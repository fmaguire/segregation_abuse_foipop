{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "f43632a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "sns.set_style('whitegrid')\n",
    "sns.set_palette('colorblind')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "3253bc69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tidy_nanonets_extractions(filepath, write=False):\n",
    "    extracted = pd.read_csv(filepath, sep=',')\n",
    "    if 'col7' in extracted.columns:\n",
    "        extracted = extracted.drop(['original_filename', 'file_url', 'platform_link', 'col1', 'col7'], axis=1)\n",
    "    else:\n",
    "        extracted = extracted.drop(['original_filename', 'file_url', 'platform_link', 'col1'], axis=1)\n",
    "    extracted.columns = ['Page', 'Confinement Reason', 'Entry', 'Exit', 'Duration', 'Ethnic Origin']\n",
    "    extracted = extracted[extracted['Confinement Reason'] != 'Confinement Reason']\n",
    "    extracted['Correctional Facility'] = filepath.split('/')[-1].split('-')[0]\n",
    "    if write:\n",
    "        extracted.to_csv(f\"nanonets_extracted_csvs/stage1_tidy/{filepath.split('/')[-1]}\", sep='\\t', index=False)\n",
    "    else:\n",
    "        return extracted\n",
    "# tidy_nanonets_extractions('nanonets_extracted_csvs/cape_breton_correctional_facility-0-pdf.csv', True)\n",
    "# tidy_nanonets_extractions('nanonets_extracted_csvs/southwest_ns_correctional_facility-0-pdf.csv', True)\n",
    "# tidy_nanonets_extractions('nanonets_extracted_csvs/northeast_ns_correctional_facility-0-pdf.csv', True)\n",
    "# tidy_nanonets_extractions('nanonets_extracted_csvs/central_ns_correctional_facility_chunk001-0-pdf.csv', True)\n",
    "# tidy_nanonets_extractions('nanonets_extracted_csvs/central_ns_correctional_facility_chunk002-0-pdf.csv', True)\n",
    "\n",
    "# data then manually tidied to remove summary tables in stage2_tidy (and missing last few records added to central chunk2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "1830deea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Page, Confinement Reason, Entry, Exit, Duration, Ethnic Origin, Correctional Facility, Duration_calc]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "def fix_ethnic_origin(x):\n",
    "    if x not in ['CAU', 'BLA', 'IND', 'ANS', 'OTH', 'UNK', 'ARAB', \"ASIA\"]:\n",
    "        x = 'Redacted'\n",
    "    return x\n",
    "\n",
    "southwest = pd.read_csv('nanonets_extracted_csvs/stage2_manual_tidy/southwest_ns_correctional_facility-0-pdf.csv', sep='\\t')\n",
    "southwest['Entry'] = pd.to_datetime(southwest['Entry'])\n",
    "southwest['Exit'] = pd.to_datetime(southwest['Exit'])\n",
    "southwest['Duration_calc'] = southwest['Exit'] - southwest['Entry']\n",
    "print(southwest[southwest['Duration'] != southwest['Duration_calc'].dt.days])\n",
    "southwest = southwest.drop('Duration_calc', axis=1)\n",
    "southwest['Ethnic Origin'] = southwest['Ethnic Origin'].apply(fix_ethnic_origin)\n",
    "southwest['Correctional Facility'] = 'Southwest NS Correctional Facility'\n",
    "southwest = southwest.drop('Page', axis=1)\n",
    "southwest.to_csv('nanonets_extracted_csvs/stage3_final_clean/southwest_correctional_data.csv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "e49235cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Page, Confinement Reason, Entry, Exit, Duration, Ethnic Origin, Correctional Facility, Duration_calc]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "capebreton = pd.read_csv('nanonets_extracted_csvs/stage2_manual_tidy/cape_breton_correctional_facility-0-pdf.csv', sep='\\t')\n",
    "capebreton['Entry'] = pd.to_datetime(capebreton['Entry'])\n",
    "capebreton['Exit'] = pd.to_datetime(capebreton['Exit'])\n",
    "capebreton['Duration'] = capebreton['Duration'].str.strip('INLOلا').str.strip().astype(int)\n",
    "capebreton['Duration_calc'] = (capebreton['Exit'] - capebreton['Entry']).dt.days\n",
    "capebreton['Ethnic Origin'] = capebreton['Ethnic Origin'].apply(fix_ethnic_origin)\n",
    "print(capebreton[capebreton['Duration'] != capebreton['Duration_calc']])\n",
    "capebreton = capebreton.drop('Duration_calc', axis=1)\n",
    "capebreton['Correctional Facility'] = \"Cape Breton Correctional Facility\"\n",
    "capebreton.to_csv('nanonets_extracted_csvs/stage3_final_clean/capebreton_correctional_data.csv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "994d832e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Page, Confinement Reason, Entry, Exit, Duration, Ethnic Origin, Correctional Facility, Duration_calc]\n",
      "Index: []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_217293/2262187057.py:16: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  northeast['Duration'] = northeast['Duration'].str.replace('00', '').str.strip('LOINلیا()').str.replace(\" \\) 6\", \"6\").str.strip().astype(int)\n"
     ]
    }
   ],
   "source": [
    "northeast = pd.read_csv('nanonets_extracted_csvs/stage2_manual_tidy/northeast_ns_correctional_facility-0-pdf.csv', sep='\\t')\n",
    "northeast = northeast[~northeast['Duration'].isna()]\n",
    "\n",
    "northeast_missing_pages = dict(zip([0,1,2,3,4,5,6], [36, 43, 74, 76, 77, 86, 109]))\n",
    "northeast_missing = pd.read_csv('nanonets_extracted_csvs/stage2_manual_tidy/re_OCR_pages_with_missing_samples/northeast_ns_correctional_facility_missing_pages.csv')\n",
    "northeast_missing = northeast_missing.drop(['original_filename', 'col1'], axis=1)\n",
    "northeast_missing.columns = ['Page', 'Confinement Reason', 'Entry', 'Exit', 'Duration', 'Ethnic Origin']\n",
    "northeast_missing = northeast_missing[northeast_missing['Confinement Reason'] != 'Confinement Reason']\n",
    "northeast_missing['Page'] = northeast_missing['Page'].apply(lambda x: northeast_missing_pages[x]) - 1\n",
    "\n",
    "northeast = pd.concat([northeast, northeast_missing])\n",
    "northeast['Entry'] = pd.to_datetime(northeast['Entry'])\n",
    "northeast['Exit'] = pd.to_datetime(northeast['Exit'])\n",
    "northeast['Ethnic Origin'] = northeast['Ethnic Origin'].apply(fix_ethnic_origin)\n",
    "\n",
    "northeast['Duration'] = northeast['Duration'].str.replace('00', '').str.strip('LOINلیا()').str.replace(\" \\) 6\", \"6\").str.strip().astype(int)\n",
    "\n",
    "northeast['Duration_calc'] = (northeast['Exit'] - northeast['Entry']).dt.days\n",
    "print(northeast[northeast['Duration'] != northeast['Duration_calc']])\n",
    "northeast['Correctional Facility'] = \"Northeast NS Correctional Facility\"\n",
    "northeast.to_csv('nanonets_extracted_csvs/stage3_final_clean/northeast_correctional_data.csv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "id": "826b6e2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "228 259 272 281 283 298 315 324 329 341 355 358 368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_217293/379395370.py:32: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  central['Duration'] = central['Duration'].str.replace('8 00', '8').str.replace('00 8', '8').str.replace('\\) 6', '6').str.strip('()LOINال').str.strip().astype(int)\n"
     ]
    }
   ],
   "source": [
    "central1 = pd.read_csv(\"nanonets_extracted_csvs/stage2_manual_tidy/central_ns_correctional_facility_chunk001-0-pdf.csv\", sep='\\t')\n",
    "central2 = pd.read_csv(\"nanonets_extracted_csvs/stage2_manual_tidy/central_ns_correctional_facility_chunk002-0-pdf.csv\", sep='\\t')\n",
    "central1['Page'].max()\n",
    "central2['Page'] = central2['Page'] + 200\n",
    "central = pd.concat([central1, central2])\n",
    "central = central[central['Confinement Reason'] != 'Entry']\n",
    "central_page_counts = central[\"Page\"].value_counts()\n",
    "central_page_counts = central_page_counts[central_page_counts != 16]\n",
    "\n",
    "central_missing_pages = sorted(set(central_page_counts.index) - set([381]))\n",
    "print(\" \".join([str(x+1) for x in central_missing_pages]))\n",
    "central_missing_pages = dict(zip(list(range(len(central_missing_pages))), central_missing_pages))\n",
    "central_missing_pages\n",
    "#print(set([x for x in range(max(central_page_counts.index))]) - set(central_page_counts.index))\n",
    "\n",
    "\n",
    "central_missing_df = pd.read_csv('nanonets_extracted_csvs/stage2_manual_tidy/re_OCR_pages_with_missing_samples/central_ns_correctional_facility_missing_pages.csv')\n",
    "central_missing_df = central_missing_df.drop(['original_filename', 'col1'], axis=1)\n",
    "central_missing_df.columns = ['Page', 'Confinement Reason', 'Entry', 'Exit', 'Duration', 'Ethnic Origin']\n",
    "central_missing_df = central_missing_df[central_missing_df['Confinement Reason'] != 'Confinement Reason']\n",
    "central_missing_df['Page'] = central_missing_df['Page'].apply(lambda x: central_missing_pages[x])\n",
    "\n",
    "central = pd.concat([central, central_missing_df])\n",
    "central = central[~central['Entry'].isna()]\n",
    "central_page_counts = central['Page'].value_counts() \n",
    "central['Confinement Reason'].value_counts()\n",
    "\n",
    "\n",
    "central['Entry'] = pd.to_datetime(central['Entry'])\n",
    "central['Exit'] = pd.to_datetime(central['Exit'])\n",
    "central['Ethnic Origin'] = central['Ethnic Origin'].apply(fix_ethnic_origin)\n",
    "central['Duration'] = central['Duration'].str.replace('8 00', '8').str.replace('00 8', '8').str.replace('\\) 6', '6').str.strip('()LOINال').str.strip().astype(int)\n",
    "\n",
    "central['Duration_calc'] = (central['Exit'] - central['Entry']).dt.days\n",
    "central[central['Duration'] != central['Duration_calc']]\n",
    "central['Correctional Facility'] = \"Central NS Correctional Facility\"\n",
    "central.to_csv('nanonets_extracted_csvs/stage3_final_clean/central_correctional_data.csv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be18b1c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
